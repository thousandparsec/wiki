== '''''Genetic Conquest''' (GC)'', an AI Client for Thousand Parsec: Proposal ==

'''Abstract'''

This document provides a general overview of the design concept of the AI client I intend to design for Thousand Parsec. The core design of the AI agent would be a genetic algorithm, which would enforce natural selection of game-hypotheses, and come up with an optimized solution. Each turn, an evolutionary cycle will take place, where populations of game-hypotheses would be simulated against several AI players, the outcome evaluated, the best ones chosen as successors for the next generation, ad nauseum. The end result would be a well-adapted game-hypothesis, the first stage of which will be communicated back to the server. This design is inherently creative and non-deterministic, since it does not rely on templates for deciding on course of action. After the core design would be coded and tested, I will use an existing client library, to create a discrete and functional AI Client. 

The project will be implemented in Java, since it is the language I'm proficient in, and hence will use the libtpproto-java library. Upon completion, the AI Client will be able to successfully play the RFTS ruleset, and with slight modifications to some of its modules, GC will be able to play any other TP ruleset. 

The main benefit to TP would be to have a capable and creative AI, which will enrich the game experience for novices, as well as weathered veterans. 



*Please feel free to make suggestions/corrections in the discussion page and in the space provided below!!

== Deliverables / Goals ==

The goal of this project is to devise and implement an efficient and capable AI Client for Thousand Parsec. The main focus I have set for myself is in designing and implementing the AI agent architecture and logic (consistent with TP03 protocol), and secondary to that is the completion of a discrete and working AI Client in time for the due date. Both goals are to be considered deliverables, but the relative importance of each is highlighted for clarity of communication. Thus, more formally the deliverables will consist of:

*Design and implement the AI agent for TP in Java.
*Collaborate on completing the libtpproto-java library with jezuch, in the direction of finalizing a working standalone AI Client.
*Create a convenient and comprehensive UI for the AI client, for pre-game adjustments. Would utilize the console as a base, and optionally include a GUI, which can be activated by a console command (not by automatic pop-up).
*Fully document the process both in javadoc, and create a separate volume of comprehensive and coherent documentation.

== Planning and Progress ==

'''Roadmap (Big Checkpoints and Detailed Breakdown)'''

* '''''Initial period of learning and Software Design''''':
* April 14 - Winning Projects announced, and all exams have finished. Begin active study of libraries, ruleset, and other relevant materials.
* April 15 -> April 30 : ADTs for the project will have been designed; the core algorithm will have been completed in pseudocode and diagram form. 
* May 1 -> May 7 : Design scheme for the Opponent Simulation Module will have been completed in pseudocode and diagram form. 
  * Goal for period: All relevant materials have been studied, and initial volume of documentation released.

* '''''Coding I''''':
* May 8 -> May 14 : Relevant classes, methods, and fields will have been created for the AI agent.
* May 15 -> May 21 : The core genetic algorithm will have been coded.
* May 22 -> 28 : The Fitness Function for the evaluation of game-hypotheses will have been devised and tested.
* May 29 -> June 4 : All coding of overhead AI agent structures will have been completed.
* June 5 -> June 11 : OSM module coding (1).
* June 12 -> June 18 : OSM module coding (2). OSM module coding will have been completed.
* June 19 -> June 25 : Thorough unit and integration tests will have been run on the AI agent.
* June 26 -> July 7 : Buffer zone for corrections, retesting, and unexpected situations.
  * Goal for period: Core genetic algorithm and AI agent ADT, as well as the OSM, will be tested and running 
  * in time for mid-term evaluations.

*'''''Coding II''''':

* July 8 -> July 14 : Work on completing libtpproto-java towards having an AI Client (1).
* July 15 -> July 21 : Work on completing libtpproto-java towards having an AI Client (2). All work on completing the library will have been completed.
* July 22 -> July 28 : Thorough unit and integration tests will have been run on the AI client.
* July 29 -> August 11 : Buffer zone for corrections, retesting, and unexpected situations.
  * Goal for period: A complete and ready to use Java AI Client will be ready. Full-scale integration testing will have been completed.

*'''''Final touches and documentation''''':
* August 12 - August 18 : Documentation will have been written and polished; code scrubbed and extensively commented.
  * Goal for period: Finish code scrubbing, and complete all relevant documentation (javadoc and a separate volume).

'''General Considerations'''

I don't see this as a May 26-August 18 project. Rather, I'll start working on it full time beginning in April, finish the core coding by August 11, and then slowly work to complete the desired set of features from then on. I definitely see this as a project I can slowly build on, rather than just a summer contract. I will be taking one university subject this summer, but I should have more than enough time for full-time development, and for studying.  I plan on spending at least 35 hrs/wk for coding, and of course many more will go on planning, reading relevant materials and communicating. There are no week-long vacations planned for the summer, although on some weekends I may be unavailable.

'''Disaster Contingencies'''

As mentioned above, I will have two high-priority obligations for the summer: this project, and a single university subject. There will be ample time to delegate to each. I do not foresee anything that can possibly interfere with the work on the project, but my preemptive strategy to counter unexpected situations would be to divide the task into three parts: first design and implement the AI agent, with reliance on the libtpproto-java library, and then work towards finishing the AI Client, using the AI agent and the libtpproto-java library, and then the final integration-testing and polishing period. If something unexpected were to occur during the summer, then at least the core features and logic would have hopefully been completed, and the code available together with documentation.

'''Work Progress Log and Communication'''

I plan to start a blog to document my progress, as well as making it a place for general thought tangents relevant to the project. I also intend to be available on the IRC channel as much as possible, and generally communicate with the other members of the TP project as much as necessary. I believe a good policy of code submission would be to do it on a regular weekly basis, but this is up to the project mentors to have final say on.

== Benefits to Thousand Parsec, and its community of Users==

A worthy AI Client would enrich the game experience in cases where only a small number of people can have a game together. This particular design (see below) ensures inherently creative solutions, which would make it hard to "crack" and trap. These features are bound to add to gameplay quality, as human players will strive not only to outsmart each other, but will also struggle with the AI player. This is a project that has much growth potential, as outlined in the relevant section below. There are a couple of advantages advantage to making it in Java: For one, there is the ease of creating comprehensive documentation (javadoc); then,there is a high cross-platform portability the client will inherently have.

== Design Principles ==

'''Design and Testing Philosophy'''

My vision is that this AI client should be highly modular, and designed from the top-down. A well planned, careful design would serve as the start point for coding. Having created the necessary stubs for classes, methods, and fields, I will proceed with a flexible, iterative approach to coding, which will allow me to have an overall vision of which data-structures should be used to ensure efficient inter-module communication, and generally low overhead. I intend to follow the timeline previously outlined, and focus on the modules which need to be completed for deadline. I intend to run unit tests twice: First, do a top-down test with dummy-values prior to implementation, and then I plan to run black-box unit tests when coding for the module is done.


'''General Execution Sequence:'''

* Turn start: receive universe object from server.
* Extract info from the universe object.
* Run core genetic algorithm, which would return the set of actions to be performed this turn.
* Communicate actions back to server.


'''Description and Merits of a Genetic Algorithm Solution'''

I chose a genetic algorithm (GA) implementation of the AI agent for several reasons. First of all, it is perfectly suited for the role in TBS games, since is involves a progressive refinement of "hypotheses", until a suitable one has been reached, in a "contemplative" manner, rather than in a quick, heuristic fashion. Basically, it will simulate and evaluate outcomes of its own actions and opponent replies for several moves ahead, progressively improving the planned actions in future moves _as a whole_ (instead of utilizing a "greedy" solution), utilizing the power of evolution to come up with good and original solutions. Since it will be looking many moves ahead, and considering the whole move-sequence at once, the solutions it comes up with aren't obvious "quick-fixes", but genuine strategy. Together with a heuristic to predict opponent-moves, this algorithm will make the AI relatively sensitive to traps, and hard to fool. Another bonus is that the solutions it comes up with will be creative ones, since there will be no hard-coded templates on which it will rely in decision making. This will ensure that it will always be a creative and unpredictable opponent! Finally, a GA design will make the algorithm highly modular, and ensure quick transitions between rulesets. 

'''Some Objections and Concerns Raised Regarding the Proposed Solution''' 

This section refers mainly to information given later on in the document, but it is appropriately placed together with the general discussion of the design. The main concern that was raised, is that the way Hypotheses are encoded does not allow for meaningful comparison between their constituents. Furthermore, it was claimed that the design does not allow for a smooth optimization curve, since action-encoding is unique, and one such action cannot be changed to represent a slight variation of itself. Both claims reflect accurate analysis of how Hypotheses will be encoded, yet they fail to perceive both the essence of the evolutionary process taking place, and the relative impact of a single action on the overall game. Let me elaborate a bit. 

First of all, the evaluative function (fitness function) will measure the outcome of the Hypothesis as a whole, with complete disregard to its parts. This has an upside and a possible downside. The upside is in the fact that non-obvious, creative strategies can be devised to counter "commonsense" players, such as set traps for them. The possible downside may be that since it is de facto impossible to predict the actions of others, it may do something suboptimal, and then fail at expecting the outcome, which will lead to a bad solution. The emergent behavioral characteristics of the AI agent would be originality, and audacity, and would make it an interesting opponent to play against. 

Secondly, it was claimed that changing a single action can significantly alter the process of the game, and thusly the algorithm will not have a smooth optimization curve, but will jump haphazardly each cycle of mutations. This is an incorrect claim when the game is considered as a whole. Each turn, a reasonable player may perform tens or hundreds of actions, some of which are more influential than others. Thus, each generation of mutations will of course have a few "outliers" as far as the optimization curve is concerned, but the majority of newly generated Hypotheses will roughly follow the curve. Another feature worth mentioning here, is that from each seed three populations will be generated, which vary on the amount of mutation that took to produce them. This will allow the algorithm to have powerful search, while maintaining a conservative population to perfect good solutions. Also, good solutions will be preserved between-generations, and between-turns, to give the natural selection process an additional layer of robustness. 

By the way, please feel free to look for the code sample below, which is a very simplified proof-of-concept application. It should decisively answer the second concern, that having unique encoding for each action, even if the presence or absence of such action can significantly alter the situation, does not compromise the optimization process.

My hope is that the above answers these concerns, and provides necessary clarification for anyone who needs it. 

If anyone wishes to reply to this, please do so in the dedicated section. 

'''Possible Alternatives to a GA Solution'''

Another possible solution to having the algorithm look several moves ahead, would entail creating an alpha-beta decision tree, then proceeding by the Paranoid Hypothesis assumption. It is likely is the only heuristic to accommodate alpha-beta pruning to a 4-player game. The "opponent" in that case would be the 3 other players, playing against you, which is usually far from the truth. This is a much more memory- and execution-time guzzling, and much less accurate, than a well designed GA.

Another alternative would be to have rules to calculate a reasonable solution a single move in advance. While this may be a good quick-fix, and be suited as a training tool for inexperienced players, it will not offer the more advanced (and intelligent) player a challenge, and furthermore its simple strategy would damage gameplay quality. In fact, the OSM (discussed below), will be mainly relying on just such a "cheap" solution.


* '''''Here you can find a visually-rich PDF document, to aid the reader in understanding the design scheme of the core genetic algorithm: '''''  [http://www.4shared.com/file/42290568/5cbdb383/EvolutionaryParadigm_2.html]


'''Opponent Simulation Module (OSM)'''

The main difficulty I expect to encounter will be in making a good heuristic for predicting opponent moves (henceforth OSM, Opponent Simulation Module). Careful analysis will have to be done to estimate what should a reasonable player do, based on its assets, and the positioning on the map relative to others. Then a small set of rules (possibly probabilistic ones) will be extracted, and implemented as the OSM. I will try my best to incorporate elements of machine learning and inference making into the module, so that the AI agent will be simulating games against opponents similar to the real ones, when deciding on strategy. A further layer of heuristics will have to be used, as the information available to the player will be incomplete. I will not be able to utilize the same genetic algorithm to calculate opponent moves, since embedded GA's tend to run out of memory very quickly (actually, the recursion would never terminate, or else a good heuristic for opponent moves will have to be implemented at some point, which makes this an exercise in futility!). 


'''The Evolutionary Paradigm'''

'''''Hypotheses: The basic objects of the algorithm:'''''

The GA will operate on "hypotheses", which are basically encoded move-sequences. Each such hypothesis will be a 4D array (x-axis: encoding moves ahead (1... search_depth); y-axis: available action categories, e.g. development, purchasing, invading, etc. The logic for that is to say, for example, I want to have a maximum of 2 technology moves, 3 industry moves, 1 send fleet move, etc.; w-axis: division by planets owned and by current fleet on the move (max. value will vary in accordance with situation); z-axis: a uniquely-encoded set of actions (one set per each category). 

Such encoding ensures complete coverage, is compact and is easy to operate upon. Every point in the 4D array corresponds to some action, in some action-class, on some planet or some fleet, in some future move. In each hypothesis, the x and y-values (number of moves, action categories) will always be fixed, the w-values will change in accordance with how many planets the player owns, and the only thing that can mutate are z-values (according to some restrictions, of course).

'''''Generations and Mutation:'''''

The GA algorithm will start with an init_seed, which is a hypothesis. At the start of the game the init_seed will be determined by the OSM, which will provide a decent strategy to build on (to save on overhead costs on running the algorithm from random values. Note that the init_seed will probably mutate beyond recognition when run through enough generations).  So basically, the starting point of the optimization process would already be a commonsense and reasonable solution. Then, at each subsequent turn, the winning hypothesis of the previous turn will be modified to serve as the next turn's init_seed. The modification will be as follows: shift all 3D <y,z,w> slices one to the left on the x-axis; generate the last one using OSM. In simpler words, I take last turn's winning hypothesis, shift it a move down, and attach a commonsense solution as the last move's set of actions. Again, this is done to minimize overhead, and it will still retain creativity beyond the OSM heuristic paradigm.

The init_seed will then mutate, such that the z-values will mutate in several random points, with compliance to restrictions, to create 3 generation_seeds. At the beginning of each generation, there will be 3 generation_seeds, to preserve a variety of solutions, but on the other hand, not be too computationally-heavy. From each such generation_seed there will evolve three populations: a risk population (high mutation-rate, to seek solutions outside the local maximum), a normal population (moderate mutation rate, with reasonable search-power of solutions), and a conservative population (low mutation rate, to preserve good solutions). Then, each hypothesis will be decoded, simulated, and evaluated. From each population, the best hypothesis will make it to the next selection (9 in all). To prevent loss of good solutions, the generation_seeds and init_seed will automatically be added to this round of selection (total: 13). Then, the 3 best ones will be selected as the next generation_seeds. 

The process of making new generations will happen a fixed number of times (for a fixed and reasonable execution-time), or until some hypothesis promises a win, and at the end returns the best one. I will have to play around with the numbers, to have the AI take less than 1 to 2 minutes to complete its turn. Then, the actions specified in the first move of the winning hypothesis are sent back to the server for execution.

'''''Fitness function (Hypothesis-assessment):'''''

A fitness function will be derived to give each hypothesis a numeric evaluation, by which they can be compared (e.g. higher = better, or closer to zero = better). This function will have as parameters the current stats of the AI player, its relative standing in the universe compared to others (number of planets, fleets, etc), and the winning conditions of the ruleset in question (here, RFTS). A heuristic will have to be used, as the information available to the player will be incomplete.


'''User Interface'''

I wish to make the AI-agent as fully configurable as possible. Although it is impossible to deterministically predict and influence its behavior, some important features can be played around with:
*Difficulty: This can simply be a function of search depth of the generated hypotheses, of the size of the generation, and/or of the number of generations the algorithm will go through, before outputting a solution.
*Changes to the three OSM's the algorithm will be simulating games against, which will alter the characteristics of the simulated opponents. This can be useful if a player wants a serious challenge. In that case, the player would tweak the opponent-module to play similarly to him(her).

The UI can either be text or GUI. A console command-line interface will serve as the base for all communication with the AI Client, but if time permits I can definitely include the option to have a console command, which invokes a GUI, for increased convenience. An important note is that I only see usefulness in a UI prior to game start, so there is no reason to keep it running during the game. Perhaps some ideas as to the usefulness of in-game communication with the AI player will surface during development.

== Future Directions for ''Genetic Conquest'' ==

Endless work can be put in optimizing an optimization algorithm :). The most interesting and promising directions for future work I can see from this disadvantaged point are:

*OSM (Opponent Simulation Module) Optimization: Quite possibly the most interesting and useful direction to consider, it to provide the OSM with advanced machine-learning capabilities, directed at analyzing visible opponent behavior. This will highly optimize the selection process of hypotheses, since the games being simulated will be with opponents very similar in behavior to real ones. To that end, neural nets could be used for a robust and generic solution. ''I would be very interested in conducting further work in that avenue!''
*Evolutionary Paradigm Optimization: The above process of natural selection is robust and efficient, but it is far from perfect. Even more creative solutions can be found, for example combining evolution and dynamic tree structures, as in having each solution thus generated branch off into separate evolutionary branches, and have a dynamic tree-structure to look n-deep, and find if a branch is worth pursuing. This creates much memory overhead, but increases the originality of solutions. Note: this suggestion is _not_ the alternative mentioned earlier, since it too considers and evaluates move-sequenecs as a whole.
*Ruleset Compliance: I intend for the design to be highly modular, so that it would be straightforward to have the AI agent comply to different rulesets. Probably the best option would be to create a library for each such ruleset, and modify some aspects of the AI accordingly (i.e. the Fitness function, define permissible actions, OSM restrictions and characteristics, and other small things, which will become clearer during implementation).
*Other? As far as I can see, these are the main avenues for improvement, although as I said above, you can work forever on optimizing an optimization algorithm.

== About me ==

My name is Victor Ivri (forum and IRC: vi1985). I'm double majoring in Computer science and Cognitive science at York U, Toronto, Canada. My main interest is in the area of AI. I'm interested in both practical implementations of various forms of AI (probabilistic logic, neural nets, and genetic algorithms, to be specific), and in the philosophical and psychological underpinnings of it. I heard of genetic algorithms some time ago in a Cognitive science conference, ran home, read about how it's done, and built a few (increasingly elaborate) examples myself. I'm really excited at this opportunity to participate in the development process of a 4X game, since I'm a huge fan of both TBS and RTS. I don't have a lot of time for game-playing at the moment, but I still occasionally meet with friends to play HMM3 into the night (or morning) :). 

I possess solid Java development skills (+2 years Java coding) in both Windows and Linux environments, a strong abstraction and problem solving ability, consider myself a good communicator, and above all, I have a very, very strong desire to succeed in this project, to learn, and to grow professionally as an AI developer, and as a member in the opensource community. Although I have rather limited experience in designing client/server applications, I _will_ learn everything that is necessary for it by the end of April, at which point it will become a non-issue.

I want to do this project because it is very interesting and challenging. It is a great opportunity for me to advance my knowledge base and practical skills in my area of interest, as well as gain experience in the opensource community, and I will work very hard to achieve a successful and working result before the deadline. In the time elapsed since I first heard of TP, I've had the pleasure to talk and exchange ideas with many of you, and I hope that our collaboration will continue into the future, since the above project is definitely a something I (and others) can build and elaborate on. Even more than that, I hope that you give me a chance to prove that my ideas and skills are an asset to Thousand Parsec.


'''Programming Example''': 
Here's a rather simple, but neat, example of genetic programming (a day's worth of hacking), which solves a class of non-trivial problems in arithmetic. It is of a quite different design than explained above, but it illustrates how "letting evolution run its course" can solve non-trivial problems. Here is the link: [http://www.4shared.com/account/file/41671485/10b80908/genArithmetic.html]. I have started slowly working on a checkers AI that implements the design above for testing purposes, but I'm afraid it won't be ready soon due to university workload.

'''Contact info''': victor.ivri AT gmail DOT. com


Thank you for considering my candidacy.

== Your Comments Here ==

Very nice proposal, the design is explained in quite a lot of detail. I think in the application you will submit thru the Google webform you can shorten it and make it more compact, but it is great to have it like this on the wiki.

You should also make timeline a bit more detailed in the application and describe GUI a bit more. A some kind of risk management section should also be there: what kind od problems could arise (in coding, life) and what are your plans to deal with these problems.

You also mention that you have already done some examples. It would be a plus if you could provide a link to the source code or examples.

Into application you could also include information if you already have a blog or some other page where you would write about your progress.

Oh yeah and write down what benefits your project brings to TP and game players. Also mention in the application how many hours a week you will be working on this and if you have any other commitments during GSoC period, Maybe you will be on vacation for a few days?

I think this is enough for now from my part. Keep up the great work!
--[[User:JLP|JLP]] 11:13, 23 March 2008 (EDT)

>>>Thank you for your suggestions, JLP. I will put some source code online, and expand on the areas that you mentioned. --[[User:Vi1985|Vi1985]] 11:59, 23 March 2008 (EDT)

>>>Done. --[[User:Vi1985|Vi1985]] 16:12, 23 March 2008 (EDT)


"there is a hard-coded number of players, which must participate in the game"

- Not exactly. This is true of the standard RFTS but is one of the differences for TP RFTS- it has no min players, only a max. Btw, I've made trivial updates to the TP RFTS wiki page.
--[[User:Xdotx|Xdotx]] 21:01, 24 March 2008 (EDT)

>>>Xdotx: Thanks for the update! I have actually caught it about 30mins ago, and made a comment about it in my Google proposal ;). I should change it here as well... --[[User:Vi1985|Vi1985]] 21:31, 24 March 2008 (EDT)

'''Nash''': 

* You mention game performance rating: 
** How is this done?
>>> Hm... Did you mean Big-O analysis? --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)

>>> If this is a question about the fitness function, then I plan on devising a formula, which will evaluate the player's standing in the game relative to others. This formula will compare known information between the players, and also it will have another layer of heuristics, which will extract "probable" info about the actual stats of various players. This information will be scaled relative to the winning conditions of the specific ruleset, to produce a one-to-one function, the value of which will serve as the "performance rating" of the player in the particular universe-snapshot. --[[User:Vi1985|vi1985]] 12:43, 27 March 2008 (EDT)

>>>If you're wondering how well the AI will fare against human players, the quick answer would be: it would act,  at the very least, as a reasonable opponent for beginners. The upper bound for AI performance is how well I will program the relevant modules (the OSM, and the fitness function to be exact), which will directly influence how solutions will be evaluated on an absolute scale, and which solution will be selected for execution. If all goes well, I expect it to be reasonably good at finding creative strategies to outsmart the actions of mediocre players. --[[User:Vi1985|vi1985]] 14:53, 28 March 2008 (EDT)

* Have you had a look at the status of libproto-java?
>>> Yes, I have extensively corresponded with jezuch, the libtpproto-java developer, about these issues, and we have agreed that I will be building on the existing library in the direction of making the AI client, in consultation and (some) collaboration with him. This is specified in the last segment of the Roadmap. --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)
** Does it do what you need, is it in a good enough status to use?
** Have you taken into account you need to be using it heavily?
* Java:
** What version? 
>>> I currently use Java SE 6 for development, and if EE will have to be used, then of course the 6th edition as well. --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)
** What platform(s) will you be testing on?
>>> I am most likely to develop it on my Windows XP, and will test it at home, as well as on the RedHat Linux machines in the university's computer-science lab. --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)
* Training: 
** How long do you expect it to train a decent AI, or even an AI that can play the game?
>>> I am not starting with a "baby-AI". I'm starting with a decent game hypothesis, and improving it through the process of evolution each turn, as new information about the universe comes in. This is _not_ a slow process; I will not need to simulate more than 1000 games. Again, my goal is between1 to 2 minutes of execution time on a decent machine. --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)
** Are you going to deliver working AIs? 
>>> The question was answered above, but I will not be "cooking" AIs at home; rather, the AI agent will dynamically evolve strategies each turn. --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)
* Are AIs going to train themselves continually throughout the game? 
>>> Yes :) Not really "train" though, as much as evolve, as this is not a machine-learning paradigm I'm using; rather, it's natural selection. --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)
** Are they going to be able to stay viable in this case?  Or if they don't train?
>>> The worst-case solutions it comes up with are going to be commonsense, reasonable solutions from the init_seed; the best case solutions are going to be creative strategies to deal with reasonable opponents. Thus, the strategy that will result will be _at least_ as viable as the commonsense solution that serves as the beginning of the evolutionary process (solutions are preserved between generations and between turns, as stated in the relevant section). --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)
* Fitness Space
** You define a 4D fitness space.  Is this really enough?  How about more complex games like a Stars! ruleset?
>>> The 4D space is just a reasonable working hypothesis for this stage of research and development; this is really not a big issue, as arrays are quick to operate on, and don't take up much space. The final design can be 5 or 6D if needed. But again, I'm aiming for a working RFTS AI. Perhaps a Stars! clone would need to be modified thusly, and include more dimensions to capture the depth of the game. --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)

>>>I hope this answers your questions. --[[User:Vi1985|vi1985]] 08:37, 27 March 2008 (EDT)